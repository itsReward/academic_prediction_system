{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Academic Status and Dropout Prediction - Model Experimentation\n",
    "\n",
    "This notebook focuses on developing, evaluating, and fine-tuning machine learning models for predicting academic status and dropout risk using the processed data from our feature engineering stage. We'll experiment with various algorithms, evaluate their performance, and select the best model for deployment.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Configuration](#1.-Setup-and-Configuration)\n",
    "2. [Loading Processed Data](#2.-Loading-Processed-Data)\n",
    "3. [Baseline Models](#3.-Baseline-Models)\n",
    "4. [Model Evaluation Framework](#4.-Model-Evaluation-Framework)\n",
    "5. [Advanced Models](#5.-Advanced-Models)\n",
    "6. [Hyperparameter Tuning](#6.-Hyperparameter-Tuning)\n",
    "7. [Ensemble Methods](#7.-Ensemble-Methods)\n",
    "8. [Model Interpretability](#8.-Model-Interpretability)\n",
    "9. [Final Model Selection](#9.-Final-Model-Selection)\n",
    "10. [Model Deployment Preparation](#10.-Model-Deployment-Preparation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Let's first import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Evaluation libraries\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Model interpretation libraries\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "\n",
    "# Import custom utility functions if any\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "# from src.models.evaluate_model import plot_confusion_matrix  # Uncomment when available\n",
    "\n",
    "# Configure visualizations\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set(style=\"whitegrid\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "\n",
    "# Set random state for reproducibility\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Processed Data\n",
    "\n",
    "Let's load the processed data created during the feature engineering phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "processed_data_dir = '../data/processed'\n",
    "models_dir = '../ml_models/trained_models'\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "# Load training and testing data\n",
    "train_data = pd.read_csv(f'{processed_data_dir}/train_data.csv')\n",
    "test_data = pd.read_csv(f'{processed_data_dir}/test_data.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n",
    "\n",
    "# Load label encoder\n",
    "with open(f'{processed_data_dir}/label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# Load feature information\n",
    "with open(f'{processed_data_dir}/feature_info.pkl', 'rb') as f:\n",
    "    feature_info = pickle.load(f)\n",
    "\n",
    "# Extract target mapping\n",
    "target_mapping = feature_info['target_mapping']\n",
    "print(\"\\nTarget class mapping:\")\n",
    "for original, encoded in target_mapping.items():\n",
    "    print(f\"{original} -> {encoded}\")\n",
    "\n",
    "# Extract features and target\n",
    "X_train = train_data.drop(columns=['Target_encoded'])\n",
    "y_train = train_data['Target_encoded']\n",
    "X_test = test_data.drop(columns=['Target_encoded'])\n",
    "y_test = test_data['Target_encoded']\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# Verify if 'Target' column exists and remove it\n",
    "if 'Target' in feature_names:\n",
    "    X_train = X_train.drop(columns=['Target'])\n",
    "    X_test = X_test.drop(columns=['Target'])\n",
    "    feature_names.remove('Target')\n",
    "\n",
    "print(f\"\\nNumber of features: {len(feature_names)}\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(pd.Series(y_train).value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\nClass distribution in testing set:\")\n",
    "print(pd.Series(y_test).value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Models\n",
    "\n",
    "Let's first establish baseline performance with simple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model performance\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name=None):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_prob = None\n",
    "    \n",
    "    # Get probability predictions if method exists\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_pred_prob = model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate training time\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    # ROC-AUC score for multi-class (if probability predictions available)\n",
    "    roc_auc = None\n",
    "    if y_pred_prob is not None:\n",
    "        try:\n",
    "            # For multi-class, use One-vs-Rest approach\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr')\n",
    "        except:\n",
    "            # If there's an issue, skip ROC-AUC calculation\n",
    "            roc_auc = None\n",
    "    \n",
    "    # Create evaluation results dictionary\n",
    "    results = {\n",
    "        'Model': model_name if model_name else model.__class__.__name__,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision (Macro)': precision_macro,\n",
    "        'Recall (Macro)': recall_macro,\n",
    "        'F1 Score (Macro)': f1_macro,\n",
    "        'ROC-AUC (OvR)': roc_auc,\n",
    "        'Training Time (s)': training_time\n",
    "    }\n",
    "    \n",
    "    return results, y_pred, y_pred_prob\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names=None, figsize=(10, 8), title=\"Confusion Matrix\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names if class_names else 'auto',\n",
    "                yticklabels=class_names if class_names else 'auto')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline models\n",
    "baseline_models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
    "    'SVM': SVC(probability=True, random_state=RANDOM_STATE),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'LDA': LinearDiscriminantAnalysis()\n",
    "}\n",
    "\n",
    "# Evaluate each baseline model\n",
    "baseline_results = []\n",
    "predictions = {}\n",
    "\n",
    "for name, model in baseline_models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    results, y_pred, y_pred_prob = evaluate_model(model, X_train, X_test, y_train, y_test, name)\n",
    "    baseline_results.append(results)\n",
    "    predictions[name] = (y_pred, y_pred_prob)\n",
    "    print(f\"Completed {name} with accuracy: {results['Accuracy']:.4f}\")\n",
    "\n",
    "# Display results in a DataFrame\n",
    "baseline_results_df = pd.DataFrame(baseline_results)\n",
    "display(baseline_results_df.sort_values(by='F1 Score (Macro)', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize baseline model performance\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Sort by F1 score\n",
    "sorted_results = baseline_results_df.sort_values(by='F1 Score (Macro)', ascending=False)\n",
    "model_names = sorted_results['Model']\n",
    "\n",
    "# Plot bar chart for different metrics\n",
    "bar_width = 0.15\n",
    "r1 = np.arange(len(model_names))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "r3 = [x + bar_width for x in r2]\n",
    "r4 = [x + bar_width for x in r3]\n",
    "\n",
    "plt.bar(r1, sorted_results['Accuracy'], width=bar_width, label='Accuracy', color='skyblue')\n",
    "plt.bar(r2, sorted_results['Precision (Macro)'], width=bar_width, label='Precision', color='lightgreen')\n",
    "plt.bar(r3, sorted_results['Recall (Macro)'], width=bar_width, label='Recall', color='salmon')\n",
    "plt.bar(r4, sorted_results['F1 Score (Macro)'], width=bar_width, label='F1 Score', color='purple')\n",
    "\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.title('Baseline Model Performance Comparison', fontsize=16)\n",
    "plt.xticks([r + bar_width*1.5 for r in range(len(model_names))], model_names, rotation=45, ha='right')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the confusion matrix for the best baseline model\n",
    "best_baseline_model = sorted_results.iloc[0]['Model']\n",
    "print(f\"Best baseline model based on F1 Score: {best_baseline_model}\")\n",
    "\n",
    "y_pred, _ = predictions[best_baseline_model]\n",
    "class_names = [label_encoder.inverse_transform([i])[0] for i in range(len(label_encoder.classes_))]\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred, class_names=class_names, \n",
    "                      title=f\"Confusion Matrix for {best_baseline_model}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation Framework\n",
    "\n",
    "Let's create a more robust evaluation framework using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform k-fold cross-validation\n",
    "def cross_validate_model(model, X, y, cv=5, scoring='f1_macro'):\n",
    "    # Create stratified k-fold\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    cv_scores = cross_val_score(model, X, y, cv=skf, scoring=scoring)\n",
    "    \n",
    "    return {\n",
    "        'mean_score': cv_scores.mean(),\n",
    "        'std_score': cv_scores.std(),\n",
    "        'all_scores': cv_scores\n",
    "    }\n",
    "\n",
    "# Apply cross-validation to baseline models\n",
    "cv_results = []\n",
    "\n",
    "for name, model in baseline_models.items():\n",
    "    print(f\"Cross-validating {name}...\")\n",
    "    cv_result = cross_validate_model(model, X_train, y_train)\n",
    "    cv_results.append({\n",
    "        'Model': name,\n",
    "        'Mean F1 Score': cv_result['mean_score'],\n",
    "        'Std F1 Score': cv_result['std_score']\n",
    "    })\n",
    "    print(f\"Completed {name} with mean F1 score: {cv_result['mean_score']:.4f} Â± {cv_result['std_score']:.4f}\")\n",
    "\n",
    "# Display cross-validation results\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "display(cv_results_df.sort_values(by='Mean F1 Score', ascending=False))\n",
    "\n",
    "# Visualize cross-validation results\n",
    "plt.figure(figsize=(12, 8))\n",
    "sorted_cv = cv_results_df.sort_values(by='Mean F1 Score', ascending=False)\n",
    "plt.bar(sorted_cv['Model'], sorted_cv['Mean F1 Score'], yerr=sorted_cv['Std F1 Score'], \n",
    "        color='skyblue', capsize=7, alpha=0.8)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Mean F1 Score')\n",
    "plt.title('Cross-Validation Results (F1 Score)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Models\n",
    "\n",
    "Let's try more sophisticated algorithms like XGBoost and LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define advanced models\n",
    "advanced_models = {\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
    "    'LightGBM': LGBMClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
    "    'Neural Network': MLPClassifier(max_iter=1000, random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "# Evaluate advanced models\n",
    "advanced_results = []\n",
    "advanced_predictions = {}\n",
    "\n",
    "for name, model in advanced_models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    results, y_pred, y_pred_prob = evaluate_model(model, X_train, X_test, y_train, y_test, name)\n",
    "    advanced_results.append(results)\n",
    "    advanced_predictions[name] = (y_pred, y_pred_prob)\n",
    "    print(f\"Completed {name} with accuracy: {results['Accuracy']:.4f}\")\n",
    "\n",
    "# Display results in a DataFrame\n",
    "advanced_results_df = pd.DataFrame(advanced_results)\n",
    "display(advanced_results_df.sort_values(by='F1 Score (Macro)', ascending=False))\n",
    "\n",
    "# Combine baseline and advanced results\n",
    "all_results_df = pd.concat([baseline_results_df, advanced_results_df])\n",
    "all_sorted = all_results_df.sort_values(by='F1 Score (Macro)', ascending=False)\n",
    "display(all_sorted.head(5))\n",
    "\n",
    "# Let's look at the confusion matrix for the best advanced model\n",
    "best_advanced_model = advanced_results_df.sort_values(by='F1 Score (Macro)', ascending=False).iloc[0]['Model']\n",
    "print(f\"Best advanced model based on F1 Score: {best_advanced_model}\")\n",
    "\n",
    "y_pred_adv, _ = advanced_predictions[best_advanced_model]\n",
    "plot_confusion_matrix(y_test, y_pred_adv, class_names=class_names, \n",
    "                      title=f\"Confusion Matrix for {best_advanced_model}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_adv, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning\n",
    "\n",
    "Let's optimize the top-performing models using hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top models for hyperparameter tuning\n",
    "top_models = all_sorted.head(3)['Model'].tolist()\n",
    "print(f\"Top models for hyperparameter tuning: {top_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tune hyperparameters using RandomizedSearchCV\n",
    "def tune_hyperparameters(model, param_grid, X, y, cv=5, n_iter=20, scoring='f1_macro'):\n",
    "    # Create randomized search\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the search\n",
    "    start_time = time.time()\n",
    "    random_search.fit(X, y)\n",
    "    tuning_time = time.time() - start_time\n",
    "    \n",
    "    # Best parameters and score\n",
    "    best_params = random_search.best_params_\n",
    "    best_score = random_search.best_score_\n",
    "    best_estimator = random_search.best_estimator_\n",
    "    \n",
    "    return {\n",
    "        'best_params': best_params,\n",
    "        'best_score': best_score,\n",
    "        'best_estimator': best_estimator,\n",
    "        'tuning_time': tuning_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grids for top models\n",
    "param_grids = {}\n",
    "\n",
    "# XGBoost parameters\n",
    "if 'XGBoost' in top_models:\n",
    "    param_grids['XGBoost'] = {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [3, 4, 5, 6, 7, 8],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'gamma': [0, 0.1, 0.2, 0.3],\n",
    "        'min_child_weight': [1, 3, 5, 7]\n",
    "    }\n",
    "\n",
    "# Random Forest parameters\n",
    "if 'Random Forest' in top_models:\n",
    "    param_grids['Random Forest'] = {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [None, 5, 10, 15, 20, 25],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    }\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "if 'Gradient Boosting' in top_models:\n",
    "    param_grids['Gradient Boosting'] = {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [3, 4, 5, 6],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "# LightGBM parameters\n",
    "if 'LightGBM' in top_models:\n",
    "    param_grids['LightGBM'] = {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [3, 4, 5, 6, 7, 8, -1],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'min_child_samples': [5, 10, 20, 30],\n",
    "        'num_leaves': [31, 50, 70, 90, 121]\n",
    "    }\n",
    "\n",
    "# Neural Network parameters\n",
    "if 'Neural Network' in top_models:\n",
    "    param_grids['Neural Network'] = {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (100, 100)],\n",
    "        'activation': ['relu', 'tanh', 'logistic'],\n",
    "        'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "        'solver': ['adam', 'sgd'],\n",
    "        'max_iter': [1000, 2000, 3000]\n",
    "    }\n",
    "\n",
    "# SVM parameters\n",
    "if 'SVM' in top_models:\n",
    "    param_grids['SVM'] = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.1, 0.01, 0.001],\n",
    "        'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "    }\n",
    "\n",
    "# Logistic Regression parameters\n",
    "if 'Logistic Regression' in top_models:\n",
    "    param_grids['Logistic Regression'] = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "        'max_iter': [100, 500, 1000]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter tuning for top models\n",
    "tuning_results = {}\n",
    "tuned_models = {}\n",
    "\n",
    "for model_name in top_models:\n",
    "    if model_name in param_grids:\n",
    "        print(f\"\\nTuning hyperparameters for {model_name}...\")\n",
    "        \n",
    "        # Get base model and parameter grid\n",
    "        if model_name == 'XGBoost':\n",
    "            base_model = XGBClassifier(random_state=RANDOM_STATE)\n",
    "        elif model_name == 'Random Forest':\n",
    "            base_model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "        elif model_name == 'Gradient Boosting':\n",
    "            base_model = GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
    "        elif model_name == 'LightGBM':\n",
    "            base_model = LGBMClassifier(random_state=RANDOM_STATE)\n",
    "        elif model_name == 'Neural Network':\n",
    "            base_model = MLPClassifier(random_state=RANDOM_STATE)\n",
    "        elif model_name == 'SVM':\n",
    "            base_model = SVC(probability=True, random_state=RANDOM_STATE)\n",
    "        elif model_name == 'Logistic Regression':\n",
    "            base_model = LogisticRegression(random_state=RANDOM_STATE)\n",
    "        \n",
    "        param_grid = param_grids[model_name]\n",
    "        \n",
    "        # Tune hyperparameters\n",
    "        tuning_result = tune_hyperparameters(base_model, param_grid, X_train, y_train)\n",
    "        tuning_results[model_name] = tuning_result\n",
    "        \n",
    "        # Save best estimator\n",
    "        tuned_models[model_name] = tuning_result['best_estimator']\n",
    "        \n",
    "        print(f\"Best F1 score for {model_name}: {tuning_result['best_score']:.4f}\")\n",
    "        print(f\"Best parameters for {model_name}:\")\n",
    "        for param, value in tuning_result['best_params'].items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "    else:\n",
    "        print(f\"No parameter grid defined for {model_name}. Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned models on test set\n",
    "tuned_results = []\n",
    "tuned_predictions = {}\n",
    "\n",
    "for name, model in tuned_models.items():\n",
    "    print(f\"Evaluating tuned {name}...\")\n",
    "    results, y_pred, y_pred_prob = evaluate_model(model, X_train, X_test, y_train, y_test, f\"Tuned {name}\")\n",
    "    tuned_results.append(results)\n",
    "    tuned_predictions[name] = (y_pred, y_pred_prob)\n",
    "    print(f\"Completed tuned {name} with accuracy: {results['Accuracy']:.4f}\")\n",
    "\n",
    "# Display results in a DataFrame\n",
    "tuned_results_df = pd.DataFrame(tuned_results)\n",
    "display(tuned_results_df.sort_values(by='F1 Score (Macro)', ascending=False))\n",
    "\n",
    "# Compare with original untuned models\n",
    "comparison_models = []\n",
    "for model_name in tuned_models.keys():\n",
    "    # Get original results\n",
    "    original_results = all_results_df[all_results_df['Model'] == model_name].to_dict('records')[0]\n",
    "    original_results['Model'] = f\"Original {model_name}\"\n",
    "    comparison_models.append(original_results)\n",
    "\n",
    "# Combine with tuned results\n",
    "comparison_df = pd.DataFrame(comparison_models + tuned_results)\n",
    "display(comparison_df.sort_values(by=['Model', 'F1 Score (Macro)'], ascending=[True, False]))\n",
    "\n",
    "# Let's look at the confusion matrix for the best tuned model\n",
    "best_tuned_model_name = tuned_results_df.sort_values(by='F1 Score (Macro)', ascending=False).iloc[0]['Model']\n",
    "print(f\"Best tuned model based on F1 Score: {best_tuned_model_name}\")\n",
    "\n",
    "best_tuned_model = best_tuned_model_name.replace('Tuned ', '')\n",
    "y_pred_tuned, _ = tuned_predictions[best_tuned_model]\n",
    "plot_confusion_matrix(y_test, y_pred_tuned, class_names=class_names, \n",
    "                      title=f\"Confusion Matrix for {best_tuned_model_name}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tuned, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ensemble Methods\n",
    "\n",
    "Let's explore ensemble methods to potentially improve performance further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a voting classifier using the best tuned models\n",
    "voting_classifiers = []\n",
    "for name, model in tuned_models.items():\n",
    "    voting_classifiers.append((name, model))\n",
    "\n",
    "# Create and evaluate the voting classifier\n",
    "if len(voting_classifiers) >= 2:  # Need at least 2 classifiers for voting\n",
    "    # Hard voting\n",
    "    voting_hard = VotingClassifier(estimators=voting_classifiers, voting='hard')\n",
    "    hard_results, hard_pred, _ = evaluate_model(voting_hard, X_train, X_test, y_train, y_test, \"Voting (Hard)\")\n",
    "    \n",
    "    # Soft voting\n",
    "    voting_soft = VotingClassifier(estimators=voting_classifiers, voting='soft')\n",
    "    soft_results, soft_pred, soft_pred_prob = evaluate_model(voting_soft, X_train, X_test, y_train, y_test, \"Voting (Soft)\")\n",
    "    \n",
    "    # Display voting results\n",
    "    voting_results_df = pd.DataFrame([hard_results, soft_results])\n",
    "    display(voting_results_df)\n",
    "    \n",
    "    # Compare with best individual model\n",
    "    best_individual = tuned_results_df.iloc[0].to_dict()\n",
    "    comparison = pd.DataFrame([best_individual, hard_results, soft_results])\n",
    "    display(comparison)\n",
    "    \n",
    "    # Plot confusion matrix for best voting method\n",
    "    if soft_results['F1 Score (Macro)'] > hard_results['F1 Score (Macro)']:\n",
    "        best_voting = \"Soft Voting\"\n",
    "        best_voting_pred = soft_pred\n",
    "    else:\n",
    "        best_voting = \"Hard Voting\"\n",
    "        best_voting_pred = hard_pred\n",
    "        \n",
    "    plot_confusion_matrix(y_test, best_voting_pred, class_names=class_names, \n",
    "                          title=f\"Confusion Matrix for {best_voting}\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"Classification Report for {best_voting}:\")\n",
    "    print(classification_report(y_test, best_voting_pred, target_names=class_names))\n",
    "else:\n",
    "    print(\"Not enough tuned models to create a voting classifier. Need at least 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stacking classifier using the tuned models\n",
    "if len(tuned_models) >= 2:  # Need at least 2 base estimators for stacking\n",
    "    # Define base estimators\n",
    "    estimators = [(name, model) for name, model in tuned_models.items()]\n",
    "    \n",
    "    # Try different meta-classifiers\n",
    "    meta_classifiers = {\n",
    "        'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
    "        'XGBoost': XGBClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "    }\n",
    "    \n",
    "    stacking_results = []\n",
    "    stacking_predictions = {}\n",
    "    \n",
    "    for meta_name, meta_clf in meta_classifiers.items():\n",
    "        print(f\"Training Stacking Classifier with {meta_name} as meta-classifier...\")\n",
    "        \n",
    "        stacking = StackingClassifier(\n",
    "            estimators=estimators,\n",
    "            final_estimator=meta_clf,\n",
    "            cv=5,\n",
    "            stack_method='predict_proba'\n",
    "        )\n",
    "        \n",
    "        results, y_pred, y_pred_prob = evaluate_model(stacking, X_train, X_test, y_train, y_test, \n",
    "                                                      f\"Stacking ({meta_name})\")\n",
    "        stacking_results.append(results)\n",
    "        stacking_predictions[meta_name] = (y_pred, y_pred_prob)\n",
    "        print(f\"Completed Stacking ({meta_name}) with accuracy: {results['Accuracy']:.4f}\")\n",
    "    \n",
    "    # Display stacking results\n",
    "    stacking_results_df = pd.DataFrame(stacking_results)\n",
    "    display(stacking_results_df.sort_values(by='F1 Score (Macro)', ascending=False))\n",
    "    \n",
    "    # Compare with best individual model and voting\n",
    "    best_meta = stacking_results_df.sort_values(by='F1 Score (Macro)', ascending=False).iloc[0]['Model']\n",
    "    best_meta_name = best_meta.replace('Stacking (', '').replace(')', '')\n",
    "    \n",
    "    # Extract best predictions\n",
    "    y_pred_stacking, _ = stacking_predictions[best_meta_name]\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(y_test, y_pred_stacking, class_names=class_names, \n",
    "                          title=f\"Confusion Matrix for {best_meta}\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"Classification Report for {best_meta}:\")\n",
    "    print(classification_report(y_test, y_pred_stacking, target_names=class_names))\n",
    "else:\n",
    "    print(\"Not enough tuned models to create a stacking classifier. Need at least 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Interpretability\n",
    "\n",
    "Let's analyze feature importance and gain insights from our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best overall model based on performance\n",
    "all_model_results = pd.concat([baseline_results_df, advanced_results_df, tuned_results_df])\n",
    "if 'voting_results_df' in locals():\n",
    "    all_model_results = pd.concat([all_model_results, voting_results_df])\n",
    "if 'stacking_results_df' in locals():\n",
    "    all_model_results = pd.concat([all_model_results, stacking_results_df])\n",
    "\n",
    "best_overall = all_model_results.sort_values(by='F1 Score (Macro)', ascending=False).iloc[0]\n",
    "best_model_name = best_overall['Model']\n",
    "print(f\"Best overall model: {best_model_name} with F1 Score: {best_overall['F1 Score (Macro)']:.4f}\")\n",
    "\n",
    "# Get the best model object for interpretation\n",
    "if best_model_name.startswith('Tuned '):\n",
    "    model_key = best_model_name.replace('Tuned ', '')\n",
    "    if model_key in tuned_models:\n",
    "        best_model = tuned_models[model_key]\n",
    "    else:\n",
    "        print(f\"Model {model_key} not found in tuned_models. Using the first tuned model.\")\n",
    "        best_model = list(tuned_models.values())[0]\n",
    "elif best_model_name in baseline_models:\n",
    "    best_model = baseline_models[best_model_name]\n",
    "elif best_model_name in advanced_models:\n",
    "    best_model = advanced_models[best_model_name]\n",
    "elif best_model_name.startswith('Voting'):\n",
    "    if 'voting_soft' in locals() and best_model_name == \"Voting (Soft)\":\n",
    "        best_model = voting_soft\n",
    "    elif 'voting_hard' in locals() and best_model_name == \"Voting (Hard)\":\n",
    "        best_model = voting_hard\n",
    "    else:\n",
    "        print(f\"Voting model {best_model_name} not found. Using first tuned model.\")\n",
    "        best_model = list(tuned_models.values())[0]\n",
    "elif best_model_name.startswith('Stacking'):\n",
    "    # Extract the meta-classifier name\n",
    "    meta_name = best_model_name.replace('Stacking (', '').replace(')', '')\n",
    "    \n",
    "    # Recreate the stacking classifier\n",
    "    estimators = [(name, model) for name, model in tuned_models.items()]\n",
    "    if meta_name in meta_classifiers:\n",
    "        best_model = StackingClassifier(\n",
    "            estimators=estimators,\n",
    "            final_estimator=meta_classifiers[meta_name],\n",
    "            cv=5,\n",
    "            stack_method='predict_proba'\n",
    "        )\n",
    "        best_model.fit(X_train, y_train)\n",
    "    else:\n",
    "        print(f\"Meta-classifier {meta_name} not found. Using first tuned model.\")\n",
    "        best_model = list(tuned_models.values())[0]\n",
    "else:\n",
    "    print(f\"Model {best_model_name} not recognized. Using the first tuned model.\")\n",
    "    best_model = list(tuned_models.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "# Method 1: Built-in feature importance (if available)\n",
    "feature_importances = None\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importances = best_model.feature_importances_\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # For linear models like Logistic Regression\n",
    "    feature_importances = np.abs(best_model.coef_).mean(axis=0) if best_model.coef_.ndim > 1 else np.abs(best_model.coef_)\n",
    "\n",
    "if feature_importances is not None:\n",
    "    # Create DataFrame of feature importances\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importances\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # Plot top 20 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
    "    plt.title(f'Top 20 Feature Importances for {best_model_name}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No built-in feature importance available for this model.\")\n",
    "    \n",
    "    # Fallback: Use permutation importance\n",
    "    print(\"Using permutation importance instead...\")\n",
    "    perm_importance = permutation_importance(best_model, X_test, y_test, \n",
    "                                            n_repeats=10, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Create DataFrame of permutation importances\n",
    "    perm_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': perm_importance.importances_mean\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    perm_importance_df = perm_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # Plot top 20 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=perm_importance_df.head(20))\n",
    "    plt.title(f'Top 20 Permutation Importances for {best_model_name}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Analysis (if compatible with the model)\n",
    "try:\n",
    "    # Try to use SHAP for model interpretation\n",
    "    # Note: This might not work for all model types\n",
    "    explainer = shap.Explainer(best_model, X_train)\n",
    "    shap_values = explainer(X_test.iloc[:100])  # Use a subset for computational efficiency\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    shap.summary_plot(shap_values, X_test.iloc[:100], plot_type=\"bar\")\n",
    "    plt.title(f'SHAP Feature Importance for {best_model_name}', fontsize=16)\n",
    "    plt.show()\n",
    "    \n",
    "    # SHAP value summary plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    shap.summary_plot(shap_values, X_test.iloc[:100])\n",
    "    plt.title(f'SHAP Summary Plot for {best_model_name}', fontsize=16)\n",
    "    plt.show()\n",
    "    \n",
    "    # SHAP dependence plots for top features\n",
    "    if feature_importances is not None:\n",
    "        top_features = importance_df.head(3)['Feature'].tolist()\n",
    "    else:\n",
    "        top_features = perm_importance_df.head(3)['Feature'].tolist()\n",
    "    \n",
    "    for feature in top_features:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        shap.dependence_plot(feature, shap_values.values, X_test.iloc[:100], feature_names=feature_names)\n",
    "        plt.title(f'SHAP Dependence Plot for {feature}', fontsize=16)\n",
    "        plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"SHAP analysis failed: {e}\")\n",
    "    print(\"SHAP may not be compatible with this model type or structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze specific examples\n",
    "# Let's look at a few examples from each class\n",
    "\n",
    "# Get predictions from best model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "if hasattr(best_model, \"predict_proba\"):\n",
    "    y_pred_proba_best = best_model.predict_proba(X_test)\n",
    "else:\n",
    "    y_pred_proba_best = None\n",
    "\n",
    "# Function to find interesting examples\n",
    "def find_interesting_examples(y_true, y_pred, proba=None, n_per_category=2):\n",
    "    results = []\n",
    "    \n",
    "    # Correct predictions with high confidence\n",
    "    correct = y_true == y_pred\n",
    "    correct_indices = np.where(correct)[0]\n",
    "    \n",
    "    # Incorrect predictions\n",
    "    incorrect = y_true != y_pred\n",
    "    incorrect_indices = np.where(incorrect)[0]\n",
    "    \n",
    "    # For each true class\n",
    "    for cls in np.unique(y_true):\n",
    "        cls_indices = np.where(y_true == cls)[0]\n",
    "        \n",
    "        # Correct predictions for this class\n",
    "        cls_correct = np.intersect1d(cls_indices, correct_indices)\n",
    "        \n",
    "        # If proba is available, find highest confidence\n",
    "        if proba is not None and len(cls_correct) > 0:\n",
    "            # Get confidence scores for the predicted class\n",
    "            confidence = [proba[i][y_pred[i]] for i in cls_correct]\n",
    "            # Sort by confidence (highest first)\n",
    "            sorted_indices = cls_correct[np.argsort(-np.array(confidence))]\n",
    "            # Take top n examples\n",
    "            high_conf_correct = sorted_indices[:n_per_category] if len(sorted_indices) >= n_per_category else sorted_indices\n",
    "        else:\n",
    "            # Just take the first n examples if no probability available\n",
    "            high_conf_correct = cls_correct[:n_per_category] if len(cls_correct) >= n_per_category else cls_correct\n",
    "        \n",
    "        for idx in high_conf_correct:\n",
    "            confidence_val = proba[idx][y_pred[idx]] if proba is not None else None\n",
    "            results.append({\n",
    "                'Index': idx,\n",
    "                'True Class': cls,\n",
    "                'Predicted Class': y_pred[idx],\n",
    "                'Confidence': confidence_val,\n",
    "                'Correct': True,\n",
    "                'Category': f\"High confidence correct prediction for class {cls}\"\n",
    "            })\n",
    "        \n",
    "        # Incorrect predictions for this class\n",
    "        cls_incorrect = np.intersect1d(cls_indices, incorrect_indices)\n",
    "        \n",
    "        if len(cls_incorrect) > 0:\n",
    "            for idx in cls_incorrect[:n_per_category]:\n",
    "                confidence_val = proba[idx][y_pred[idx]] if proba is not None else None\n",
    "                results.append({\n",
    "                    'Index': idx,\n",
    "                    'True Class': cls,\n",
    "                    'Predicted Class': y_pred[idx],\n",
    "                    'Confidence': confidence_val,\n",
    "                    'Correct': False,\n",
    "                    'Category': f\"Incorrect prediction for class {cls}\"\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Find interesting examples\n",
    "examples = find_interesting_examples(y_test, y_pred_best, y_pred_proba_best)\n",
    "\n",
    "# Map encoded class values back to original class names\n",
    "examples['True Class Name'] = examples['True Class'].apply(lambda x: label_encoder.inverse_transform([x])[0])\n",
    "examples['Predicted Class Name'] = examples['Predicted Class'].apply(lambda x: label_encoder.inverse_transform([x])[0])\n",
    "\n",
    "# Display examples\n",
    "display(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Model Selection\n",
    "\n",
    "Based on the comprehensive evaluation, let's select the final model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize all model performances\n",
    "print(\"Model Performance Summary:\")\n",
    "display(all_model_results.sort_values(by='F1 Score (Macro)', ascending=False).head(10))\n",
    "\n",
    "# Select final model\n",
    "final_model = best_model\n",
    "final_model_name = best_model_name\n",
    "\n",
    "print(f\"\\nSelected Final Model: {final_model_name}\")\n",
    "print(f\"F1 Score (Macro): {best_overall['F1 Score (Macro)']:.4f}\")\n",
    "print(f\"Accuracy: {best_overall['Accuracy']:.4f}\")\n",
    "\n",
    "# Get predictions from final model\n",
    "final_predictions = best_model.predict(X_test)\n",
    "if hasattr(best_model, \"predict_proba\"):\n",
    "    final_probabilities = best_model.predict_proba(X_test)\n",
    "else:\n",
    "    final_probabilities = None\n",
    "\n",
    "# Print classification report for final model\n",
    "print(\"\\nClassification Report for Final Model:\")\n",
    "print(classification_report(y_test, final_predictions, target_names=class_names))\n",
    "\n",
    "# Plot confusion matrix for final model\n",
    "plot_confusion_matrix(y_test, final_predictions, class_names=class_names, \n",
    "                     title=f\"Confusion Matrix for Final Model ({final_model_name})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Deployment Preparation\n",
    "\n",
    "Let's prepare the final model for deployment by saving it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model_output_path = f\"{models_dir}/final_model.pkl\"\n",
    "with open(model_output_path, 'wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "print(f\"Final model saved to {model_output_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_name': final_model_name,\n",
    "    'creation_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'performance': {\n",
    "        'accuracy': float(best_overall['Accuracy']),\n",
    "        'precision_macro': float(best_overall['Precision (Macro)']),\n",
    "        'recall_macro': float(best_overall['Recall (Macro)']),\n",
    "        'f1_macro': float(best_overall['F1 Score (Macro)']),\n",
    "        'roc_auc': float(best_overall['ROC-AUC (OvR)']) if best_overall['ROC-AUC (OvR)'] is not None else None\n",
    "    },\n",
    "    'feature_names': feature_names,\n",
    "    'target_mapping': {str(k): int(v) for k, v in target_mapping.items()},\n",
    "    'class_names': class_names,\n",
    "    'model_type': type(final_model).__name__\n",
    "}\n",
    "\n",
    "metadata_output_path = f\"{models_dir}/model_metadata.json\"\n",
    "import json\n",
    "with open(metadata_output_path, 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=4)\n",
    "print(f\"Model metadata saved to {metadata_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simplified prediction function for deployment\n",
    "def predict_dropout_risk(data, model=final_model, encoder=label_encoder, feature_list=feature_names):\n",
    "    \"\"\"\n",
    "    Make academic status predictions with the trained model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        Data containing the required features for prediction\n",
    "    model : trained model\n",
    "        The trained classifier model\n",
    "    encoder : LabelEncoder\n",
    "        Encoder used to transform class labels\n",
    "    feature_list : list\n",
    "        List of features expected by the model\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict containing:\n",
    "        - predicted_class: The predicted academic status\n",
    "        - probabilities: Probability scores for each class (if available)\n",
    "    \"\"\"\n",
    "    # Ensure all required features are present\n",
    "    missing_features = set(feature_list) - set(data.columns)\n",
    "    if missing_features:\n",
    "        raise ValueError(f\"Missing required features: {missing_features}\")\n",
    "    \n",
    "    # Extract features in the correct order\n",
    "    X = data[feature_list]\n",
    "    \n",
    "    # Make predictions\n",
    "    predicted_class_encoded = model.predict(X)\n",
    "    predicted_class = encoder.inverse_transform(predicted_class_encoded)\n",
    "    \n",
    "    # Get probability scores if available\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        probabilities = model.predict_proba(X)\n",
    "        class_probs = {encoder.inverse_transform([i])[0]: probabilities[0][i] \n",
    "                       for i in range(len(encoder.classes_))}\n",
    "    else:\n",
    "        class_probs = None\n",
    "    \n",
    "    return {\n",
    "        'predicted_class': predicted_class[0],\n",
    "        'probabilities': class_probs\n",
    "    }\n",
    "\n",
    "# Save the prediction function\n",
    "prediction_function_path = f\"{models_dir}/predict_function.py\"\n",
    "with open(prediction_function_path, 'w') as f:\n",
    "    f.write(inspect.getsource(predict_dropout_risk))\n",
    "print(f\"Prediction function saved to {prediction_function_path}\")\n",
    "\n",
    "# Test the prediction function on a sample\n",
    "sample_data = X_test.iloc[[0]]\n",
    "sample_prediction = predict_dropout_risk(sample_data)\n",
    "print(\"\\nSample Prediction:\")\n",
    "print(f\"Predicted Class: {sample_prediction['predicted_class']}\")\n",
    "if sample_prediction['probabilities']:\n",
    "    print(\"Class Probabilities:\")\n",
    "    for cls, prob in sample_prediction['probabilities'].items():\n",
    "        print(f\"  {cls}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "In this notebook, we performed comprehensive model experimentation for our academic status and dropout prediction system. Here's a summary of what we accomplished:\n",
    "\n",
    "1. **Model Evaluation**:\n",
    "   - Implemented and evaluated multiple classification algorithms\n",
    "   - Compared model performance using accuracy, precision, recall, and F1 score\n",
    "   - Validated models using cross-validation\n",
    "\n",
    "2. **Advanced Techniques**:\n",
    "   - Tuned hyperparameters for top-performing models\n",
    "   - Explored ensemble methods including voting and stacking\n",
    "   - Achieved improved performance through model optimization\n",
    "\n",
    "3. **Model Interpretability**:\n",
    "   - Analyzed feature importance to understand key predictors\n",
    "   - Used SHAP values to interpret model decisions\n",
    "   - Examined specific examples to gain insights into model behavior\n",
    "\n",
    "4. **Deployment Preparation**:\n",
    "   - Selected the best-performing model for deployment\n",
    "   - Saved the model and its metadata for future use\n",
    "   - Created a simplified prediction function for integration\n",
    "\n",
    "**Next Steps**:\n",
    "1. Integrate the model into a production pipeline\n",
    "2. Develop a monitoring system to track model performance over time\n",
    "3. Create a user interface for educational stakeholders\n",
    "4. Implement feedback loops to improve model performance with new data\n",
    "\n",
    "The final model demonstrates strong predictive performance for academic status and dropout risk, with important insights into the key factors influencing student outcomes. By deploying this model in an educational setting, institutions can identify at-risk students earlier and implement targeted interventions to improve retention and academic success."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}