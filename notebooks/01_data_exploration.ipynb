{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Academic Status and Dropout Prediction - Data Exploration\n",
    "\n",
    "This notebook explores the dataset used for predicting academic status and dropout rates of students in higher education institutions. The goal is to understand the data, identify patterns, and gain insights for feature engineering and model development.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Configuration](#1.-Setup-and-Configuration)\n",
    "2. [Data Loading and Overview](#2.-Data-Loading-and-Overview)\n",
    "3. [Data Cleaning and Preprocessing](#3.-Data-Cleaning-and-Preprocessing)\n",
    "4. [Exploratory Data Analysis](#4.-Exploratory-Data-Analysis)\n",
    "   - [Univariate Analysis](#4.1-Univariate-Analysis)\n",
    "   - [Bivariate Analysis](#4.2-Bivariate-Analysis)\n",
    "   - [Multivariate Analysis](#4.3-Multivariate-Analysis)\n",
    "5. [Correlation Analysis](#5.-Correlation-Analysis)\n",
    "6. [Target Analysis](#6.-Target-Analysis)\n",
    "7. [Feature Importance](#7.-Feature-Importance)\n",
    "8. [Findings and Recommendations](#8.-Findings-and-Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Let's first import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "# Import custom utility functions if any\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "# from src.data.data_utilities import load_dataset  # Uncomment when available\n",
    "\n",
    "# Configure visualizations\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set(style=\"whitegrid\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Overview\n",
    "\n",
    "Let's load the dataset and get a general overview of its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = '../data/raw/dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of samples: {df.shape[0]}\")\n",
    "print(f\"Number of features: {df.shape[1] - 1}\")  # Excluding target column\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "df.describe(include='all').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable Distribution\n",
    "\n",
    "Let's examine the distribution of our target variable 'Target' which represents the academic status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display target variable distribution\n",
    "target_counts = df['Target'].value_counts()\n",
    "print(\"Target Distribution:\")\n",
    "print(target_counts)\n",
    "\n",
    "# Visualize target distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(x='Target', data=df)\n",
    "plt.title('Target Distribution', fontsize=16)\n",
    "plt.xlabel('Academic Status', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "\n",
    "# Add percentages on top of bars\n",
    "total = len(df)\n",
    "for p in ax.patches:\n",
    "    percentage = f'{100 * p.get_height() / total:.1f}%'\n",
    "    x = p.get_x() + p.get_width() / 2\n",
    "    y = p.get_height()\n",
    "    ax.annotate(percentage, (x, y), ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Preprocessing\n",
    "\n",
    "Let's check for missing values, duplicates, and any other data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_data = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "# Display features with missing values (if any)\n",
    "print(\"Features with missing values:\")\n",
    "display(missing_data[missing_data['Missing Values'] > 0].sort_values(by='Missing Values', ascending=False))\n",
    "\n",
    "# If all features have no missing values\n",
    "if missing_data['Missing Values'].sum() == 0:\n",
    "    print(\"No missing values found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(\"\\nSample of duplicate rows:\")\n",
    "    display(df[df.duplicated(keep='first')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Features Dictionary\n",
    "\n",
    "Let's organize our features by their types to facilitate further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize features by type\n",
    "cat_features = [\n",
    "    'Marital status', 'Application mode', 'Course',\n",
    "    'Daytime/evening attendance', 'Previous qualification', 'Nacionality',\n",
    "    'Mother\\'s qualification', 'Father\\'s qualification', \n",
    "    'Mother\\'s occupation', 'Father\\'s occupation',\n",
    "    'Displaced', 'Educational special needs', 'Debtor',\n",
    "    'Tuition fees up to date', 'Gender', 'Scholarship holder',\n",
    "    'International'\n",
    "]\n",
    "\n",
    "num_features = [\n",
    "    'Application order', 'Age at enrollment',\n",
    "    'Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)',\n",
    "    'Curricular units 1st sem (evaluations)', 'Curricular units 1st sem (approved)',\n",
    "    'Curricular units 1st sem (grade)', 'Curricular units 1st sem (without evaluations)',\n",
    "    'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (enrolled)',\n",
    "    'Curricular units 2nd sem (evaluations)', 'Curricular units 2nd sem (approved)',\n",
    "    'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)',\n",
    "    'Unemployment rate', 'Inflation rate', 'GDP'\n",
    "]\n",
    "\n",
    "target = 'Target'\n",
    "\n",
    "# Verify that all features are accounted for\n",
    "all_features = cat_features + num_features + [target]\n",
    "missing_features = set(df.columns) - set(all_features)\n",
    "extra_features = set(all_features) - set(df.columns)\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"Missing features in our categorization: {missing_features}\")\n",
    "if extra_features:\n",
    "    print(f\"Extra features in our categorization: {extra_features}\")\n",
    "\n",
    "print(f\"Categorical features: {len(cat_features)}\")\n",
    "print(f\"Numerical features: {len(num_features)}\")\n",
    "print(f\"Total features: {len(cat_features) + len(num_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis\n",
    "\n",
    "### 4.1 Univariate Analysis\n",
    "\n",
    "#### Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot categorical feature distributions\n",
    "def plot_categorical_distributions(df, features, rows=3, cols=3):\n",
    "    plt.figure(figsize=(18, 15))\n",
    "    for i, feature in enumerate(features, 1):\n",
    "        if i <= rows * cols:\n",
    "            plt.subplot(rows, cols, i)\n",
    "            value_counts = df[feature].value_counts().sort_index()\n",
    "            value_counts.plot(kind='bar')\n",
    "            plt.title(f'Distribution of {feature}')\n",
    "            plt.ylabel('Count')\n",
    "            plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot distributions for first 9 categorical features\n",
    "plot_categorical_distributions(df, cat_features[:9])\n",
    "\n",
    "# Plot distributions for remaining categorical features\n",
    "remaining_cat_features = cat_features[9:]\n",
    "if remaining_cat_features:\n",
    "    plot_categorical_distributions(df, remaining_cat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot numerical feature distributions\n",
    "def plot_numerical_distributions(df, features, rows=3, cols=3):\n",
    "    plt.figure(figsize=(18, 15))\n",
    "    for i, feature in enumerate(features, 1):\n",
    "        if i <= rows * cols:\n",
    "            plt.subplot(rows, cols, i)\n",
    "            sns.histplot(df[feature], kde=True)\n",
    "            plt.title(f'Distribution of {feature}')\n",
    "            plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot distributions for first 9 numerical features\n",
    "plot_numerical_distributions(df, num_features[:9])\n",
    "\n",
    "# Plot distributions for remaining numerical features\n",
    "remaining_num_features = num_features[9:]\n",
    "if remaining_num_features:\n",
    "    plot_numerical_distributions(df, remaining_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for numerical features to detect outliers\n",
    "def plot_boxplots(df, features, rows=3, cols=3):\n",
    "    plt.figure(figsize=(18, 15))\n",
    "    for i, feature in enumerate(features, 1):\n",
    "        if i <= rows * cols:\n",
    "            plt.subplot(rows, cols, i)\n",
    "            sns.boxplot(x=df[feature])\n",
    "            plt.title(f'Boxplot of {feature}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot boxplots for first 9 numerical features\n",
    "plot_boxplots(df, num_features[:9])\n",
    "\n",
    "# Plot boxplots for remaining numerical features\n",
    "if remaining_num_features:\n",
    "    plot_boxplots(df, remaining_num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Bivariate Analysis\n",
    "\n",
    "Let's explore the relationship between features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features vs Target\n",
    "def plot_categorical_vs_target(df, features, target, rows=3, cols=2):\n",
    "    plt.figure(figsize=(18, 15))\n",
    "    for i, feature in enumerate(features, 1):\n",
    "        if i <= rows * cols:\n",
    "            plt.subplot(rows, cols, i)\n",
    "            \n",
    "            # Create a crosstab to calculate percentages\n",
    "            ct = pd.crosstab(df[feature], df[target], normalize='index') * 100\n",
    "            ct.plot(kind='bar', stacked=True)\n",
    "            \n",
    "            plt.title(f'{feature} vs {target}')\n",
    "            plt.ylabel('Percentage (%)')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.legend(title=target)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot first 6 categorical features vs target\n",
    "plot_categorical_vs_target(df, cat_features[:6], target)\n",
    "\n",
    "# Plot next 6 categorical features vs target\n",
    "if len(cat_features) > 6:\n",
    "    plot_categorical_vs_target(df, cat_features[6:12], target)\n",
    "\n",
    "# Plot remaining categorical features vs target\n",
    "remaining_cat = cat_features[12:]\n",
    "if remaining_cat:\n",
    "    plot_categorical_vs_target(df, remaining_cat, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features vs Target (Box plots)\n",
    "def plot_numerical_vs_target(df, features, target, rows=3, cols=2):\n",
    "    plt.figure(figsize=(18, 15))\n",
    "    for i, feature in enumerate(features, 1):\n",
    "        if i <= rows * cols:\n",
    "            plt.subplot(rows, cols, i)\n",
    "            sns.boxplot(x=target, y=feature, data=df)\n",
    "            plt.title(f'{feature} by {target}')\n",
    "            plt.xlabel(target)\n",
    "            plt.ylabel(feature)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot first 6 numerical features vs target\n",
    "plot_numerical_vs_target(df, num_features[:6], target)\n",
    "\n",
    "# Plot next 6 numerical features vs target\n",
    "if len(num_features) > 6:\n",
    "    plot_numerical_vs_target(df, num_features[6:12], target)\n",
    "\n",
    "# Plot remaining numerical features vs target\n",
    "remaining_num = num_features[12:]\n",
    "if remaining_num:\n",
    "    plot_numerical_vs_target(df, remaining_num, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Multivariate Analysis\n",
    "\n",
    "Let's examine relationships between multiple variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots for academic performance features\n",
    "academic_features = [\n",
    "    'Curricular units 1st sem (approved)',\n",
    "    'Curricular units 1st sem (grade)',\n",
    "    'Curricular units 2nd sem (approved)',\n",
    "    'Curricular units 2nd sem (grade)'\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.pairplot(df[academic_features + [target]], hue=target, diag_kind='kde')\n",
    "plt.suptitle('Multivariate Analysis of Academic Performance', y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Economic indicators and their relationship with target\n",
    "economic_features = ['Unemployment rate', 'Inflation rate', 'GDP']\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.pairplot(df[economic_features + [target]], hue=target, diag_kind='kde')\n",
    "plt.suptitle('Multivariate Analysis of Economic Indicators', y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis\n",
    "\n",
    "Let's analyze correlations between numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical features\n",
    "corr_matrix = df[num_features].corr()\n",
    "\n",
    "plt.figure(figsize=(16, 14))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            vmin=-1, vmax=1, linewidths=0.5, cbar_kws={'shrink': .8})\n",
    "plt.title('Correlation Matrix of Numerical Features', fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify highly correlated features\n",
    "threshold = 0.7\n",
    "high_corr = {}\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "            col_i = corr_matrix.columns[i]\n",
    "            col_j = corr_matrix.columns[j]\n",
    "            high_corr[f\"{col_i} & {col_j}\"] = corr_matrix.iloc[i, j]\n",
    "\n",
    "if high_corr:\n",
    "    print(\"Highly correlated features (|correlation| > 0.7):\")\n",
    "    for pair, corr_val in sorted(high_corr.items(), key=lambda x: abs(x[1]), reverse=True):\n",
    "        print(f\"{pair}: {corr_val:.2f}\")\n",
    "else:\n",
    "    print(\"No highly correlated features found (threshold = 0.7)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Square Test for Categorical Features vs Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square test for categorical features\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "chi2_results = []\n",
    "\n",
    "for feature in cat_features:\n",
    "    contingency_table = pd.crosstab(df[feature], df[target])\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    chi2_results.append({\n",
    "        'Feature': feature,\n",
    "        'Chi-Square': chi2,\n",
    "        'P-Value': p,\n",
    "        'Significant': p < 0.05\n",
    "    })\n",
    "\n",
    "chi2_df = pd.DataFrame(chi2_results).sort_values(by='P-Value')\n",
    "print(\"Chi-Square Test for Independence between Categorical Features and Target:\")\n",
    "display(chi2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA Test for Numerical Features vs Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA test for numerical features\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "anova_results = []\n",
    "\n",
    "for feature in num_features:\n",
    "    # Create groups based on target values\n",
    "    groups = [df[df[target] == val][feature].dropna() for val in df[target].unique()]\n",
    "    \n",
    "    # Run ANOVA\n",
    "    f_stat, p_value = f_oneway(*groups)\n",
    "    \n",
    "    anova_results.append({\n",
    "        'Feature': feature,\n",
    "        'F-Statistic': f_stat,\n",
    "        'P-Value': p_value,\n",
    "        'Significant': p_value < 0.05\n",
    "    })\n",
    "\n",
    "anova_df = pd.DataFrame(anova_results).sort_values(by='P-Value')\n",
    "print(\"ANOVA Test for Numerical Features vs Target:\")\n",
    "display(anova_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Target Analysis\n",
    "\n",
    "Let's analyze the target variable in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top features by target class\n",
    "def plot_feature_by_target(df, feature, target):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # If feature is categorical\n",
    "    if feature in cat_features:\n",
    "        ct = pd.crosstab(df[feature], df[target], normalize='index') * 100\n",
    "        ct.plot(kind='bar', stacked=True)\n",
    "        plt.ylabel('Percentage (%)')\n",
    "    # If feature is numerical\n",
    "    else:\n",
    "        for i, target_val in enumerate(df[target].unique()):\n",
    "            subset = df[df[target] == target_val][feature]\n",
    "            sns.kdeplot(subset, label=f'{target}={target_val}')\n",
    "        plt.ylabel('Density')\n",
    "    \n",
    "    plt.title(f'{feature} by {target}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.legend(title=target)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Select top features based on ANOVA and Chi-Square tests\n",
    "top_num_features = anova_df.head(3)['Feature'].tolist()\n",
    "top_cat_features = chi2_df.head(3)['Feature'].tolist()\n",
    "\n",
    "# Plot top numerical features\n",
    "for feature in top_num_features:\n",
    "    plot_feature_by_target(df, feature, target)\n",
    "\n",
    "# Plot top categorical features\n",
    "for feature in top_cat_features:\n",
    "    plot_feature_by_target(df, feature, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance\n",
    "\n",
    "Let's get a preliminary idea of feature importance using a simple Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic feature importance with Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Prepare data for modeling\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Encode target if needed\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Create preprocessor\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_features),\n",
    "        ('cat', categorical_transformer, cat_features)\n",
    "    ])\n",
    "\n",
    "# Create and fit model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "model.fit(X, y_encoded)\n",
    "\n",
    "# Extract feature names after one-hot encoding\n",
    "ohe = model.named_steps['preprocessor'].named_transformers_['cat']\n",
    "cat_feature_names = ohe.named_steps['onehot'].get_feature_names_out(cat_features)\n",
    "feature_names = np.concatenate([num_features, cat_feature_names])\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.named_steps['classifier'].feature_importances_\n",
    "\n",
    "# Create dataframe of feature importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Display top 20 features\n",
    "top_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(20)\n",
    "display(top_features)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=top_features)\n",
    "plt.title('Top 20 Feature Importances', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Findings and Recommendations\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Based on the exploratory data analysis, we can make the following observations:\n",
    "\n",
    "1. **Target Distribution**:\n",
    "   - [Fill in after running the notebook]\n",
    "\n",
    "2. **Important Categorical Features**:\n",
    "   - [Fill in after running the notebook]\n",
    "\n",
    "3. **Important Numerical Features**:\n",
    "   - [Fill in after running the notebook]\n",
    "\n",
    "4. **Correlations**:\n",
    "   - [Fill in after running the notebook]\n",
    "\n",
    "5. **Feature Importance**:\n",
    "   - [Fill in after running the notebook]\n",
    "\n",
    "### Recommendations for Feature Engineering\n",
    "\n",
    "Based on our analysis, we recommend the following feature engineering steps:\n",
    "\n",
    "1. **Feature Selection**:\n",
    "   - Consider using the top features identified through statistical tests and feature importance.\n",
    "   - Remove or combine highly correlated features to reduce multicollinearity.\n",
    "\n",
    "2. **Feature Transformation**:\n",
    "   - Apply appropriate scaling to numerical features.\n",
    "   - Consider log transformation for skewed numerical features.\n",
    "\n",
    "3. **Feature Creation**:\n",
    "   - Create academic performance indicators by combining semester data.\n",
    "   - Consider creating interaction features between economic indicators and academic performance.\n",
    "   - Develop a socioeconomic status indicator based on parental education and occupation.\n",
    "\n",
    "4. **Dimensionality Reduction**:\n",
    "   - Consider using PCA or other dimensionality reduction techniques if needed.\n",
    "   - Group categorical levels that have similar target distributions.\n",
    "\n",
    "These recommendations will be implemented in the feature engineering notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}